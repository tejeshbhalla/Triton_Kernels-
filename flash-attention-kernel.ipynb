{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-02T09:33:13.206406Z","iopub.execute_input":"2025-09-02T09:33:13.206853Z","iopub.status.idle":"2025-09-02T09:33:13.212024Z","shell.execute_reply.started":"2025-09-02T09:33:13.206828Z","shell.execute_reply":"2025-09-02T09:33:13.211283Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport triton\nimport triton.language as tl ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T09:33:15.107496Z","iopub.execute_input":"2025-09-02T09:33:15.107780Z","iopub.status.idle":"2025-09-02T09:33:15.111798Z","shell.execute_reply.started":"2025-09-02T09:33:15.107757Z","shell.execute_reply":"2025-09-02T09:33:15.110999Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"@triton.jit \ndef attn_impl(Q,K,V,softmax_scale,M,O,stride_Q_batch,\n            stride_Q_head ,\n            stride_Q_seq ,\n            stride_Q_dim ,\n            stride_K_batch ,\n            stride_K_head,\n            stride_K_seq ,\n            stride_K_dim,\n            stride_V_batch,\n            stride_V_head,\n            stride_V_seq,\n            stride_V_dim,\n            stride_O_batch,\n            stride_O_head,\n            stride_O_seq,\n            stride_O_dim,\n            BATCH_SIZE,\n            NUM_HEADS:tl.constexpr,\n            SEQ_LEN:tl.constexpr,\n            HEAD_DIM:tl.constexpr,\n            BLOCK_SIZE_Q: tl.constexpr,\n            BLOCK_SIZE_KV: tl.constexpr,\n            STAGE :tl.constexpr, \n               ):\n    \"\"\"\n    Q - query matrix of shape (bs,num_heads,seq_len,head_dim)\n    K AND V OF same shape \n    M - for storing maximums \n    O - for storing output of PV where p is the attn-meatrix\n    BATCH_SIZE: int for the batch of sequences \n    NUM_HEADS : int for number of heafs \n    SEQ_LEN : no of tokens\n    HEAD_DIM : per head dim of token genrally ndim/num_heads \n    BLOCK_SIZE_Q : the number of tokens processed per block so we take block size of tokens \n    BLOCK_SIZE_KV : the blocks of kv to process per query per loop iteration (note we need all KV for quries anyway but here we say for per iter in a sm we load KV blocks instead of loading them fully)\n    STAGE: stage telling us if we are doing causal or non causal attention \n    \"\"\"\n    #here we assert to make sure we dont load more than \n    tl.static_assert(BLOCK_SIZE_KV <= SEQ_LEN)\n    block_index_q = tl.program_id(0) # if u remember we do grid launch with (cdiv(seq-len,block_size,q),bs*n_heads)\n    index_batch_head = tl.program_id(1) #tells us what head we are on in what batch so 0 saying batch 0 head 0 , 1 saying batch 0 head 1 ... \n    index_batch = index_batch_head // NUM_HEADS  #what batch are we processing\n    index_head  = index_batch_head % NUM_HEADS #what index we are in for a head in a batch \n\n    qvk_offset = (\n        index_batch.to(tl.int64)*stride_Q_batch+ index_head.to(tl.int64)*stride_Q_head\n    ) #to land the pointer exactly where we want to be so moving in batches and heads to make q point to correct location \n\n    Q_block_ptr  = tl.make_block_ptr(\n        base = Q + qvk_offset , #this points to a correct start of a head and batch we wanna be in\n        shape = (SEQ_LEN,HEAD_DIM),\n        strides = (stride_Q_seq,stride_Q_dim),\n        offsets = (block_index_q*BLOCK_SIZE_Q,0),\n        block_shape = (BLOCK_SIZE_Q,HEAD_DIM),\n        order = (1,0),)\n    \n    V_block_ptr  = tl.make_block_ptr(\n        base  = V+ qvk_offset, \n        shape = (SEQ_LEN,HEAD_DIM),\n        strides = (stride_V_seq,stride_V_dim),\n        offsets = (0,0),\n        block_shape = (BLOCK_SIZE_KV,HEAD_DIM),\n        order = (1,0),\n        \n    )\n        \n    K_block_ptr  = tl.make_block_ptr(\n        base  = K + qvk_offset, \n        shape = (HEAD_DIM,SEQ_LEN),\n        strides = (stride_K_dim,stride_K_seq),\n        offsets = (0,0),\n        block_shape = (HEAD_DIM,BLOCK_SIZE_KV),\n        order = (0,1),\n        \n    )\n    O_block_ptr  = tl.make_block_ptr(\n        base =  O + qvk_offset , #this points to a correct start of a head and batch we wanna be in\n        shape = (SEQ_LEN,HEAD_DIM),\n        strides = (stride_O_seq,stride_O_dim),\n        offsets = (block_index_q*BLOCK_SIZE_Q,0),\n        block_shape = (BLOCK_SIZE_Q,HEAD_DIM),\n        order = (1,0))\n    offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0,BLOCK_SIZE_Q) #these are the queries we load in above block_ptr \n    offs_kv = tl.arange(0,BLOCK_SIZE_KV) #we load all keys so index represente key tokens but we use all for both kv\n    m_i = tl.zeros([BLOCK_SIZE_Q],dtype=tl.float16)-float('inf') \n    l_i = tl.zeros([BLOCK_SIZE_Q],dtype=tl.float16)  #vector of shape block_sizeq to store sum and max\n    O_block = tl.zeros([BLOCK_SIZE_Q,HEAD_DIM],dtype=tl.float16)\n\n    Q_block = tl.load(Q_block_ptr)\n\n    #stage 1 of causal attending to all keys before query block \n    past_hi = (block_index_q + 1) * BLOCK_SIZE_Q\n    start_kv = 0\n    while start_kv < past_hi and start_kv < SEQ_LEN:\n        start_kv = tl.multiple_of(start_kv,BLOCK_SIZE_KV)\n        end_kv = start_kv + BLOCK_SIZE_KV #for start_kv 0 its BLOCK_SIZE_KV for ex(0+64) = 64\n        need_causal_mask = end_kv > block_index_q * BLOCK_SIZE_Q \n        \n        K_block = tl.load(K_block_ptr)\n        QK_block = tl.dot(Q_block , K_block)\n        QK_block = QK_block * softmax_scale \n\n        if need_causal_mask:\n            offs_kv_current = start_kv + tl.arange(0,BLOCK_SIZE_KV)\n            mask = offs_q[:,None] >= offs_kv_current[None,:]\n            QK_block = tl.where(mask,QK_block,float('-inf'))\n\n        \n        m_ij = tl.maximum(m_i,tl.max(QK_block,1))\n        QK_block = QK_block - m_ij[:,None] #bq,bk - [bq,1] #(bq,bk) - m_ij # block_q \n        P_block = tl.exp(QK_block)\n        l_ij = tl.sum(P_block,1)\n\n        alpha = tl.exp(m_i-m_ij) #0 if both same \n        l_i = l_i * alpha + l_ij\n        m_i = m_ij\n        V_block = tl.load(V_block_ptr)\n        O_block  = O_block * alpha[:,None]\n        O_block = tl.dot(P_block,V_block,O_block)\n\n        V_block_ptr = tl.advance(V_block_ptr,(BLOCK_SIZE_KV,0))\n        K_block_ptr = tl.advance(K_block_ptr,(0,BLOCK_SIZE_KV))\n\n        \n        start_kv += BLOCK_SIZE_KV\n\n    l_i = l_i + 1e-6\n    O_block = O_block / l_i[:,None]\n\n    tl.store(O_block_ptr, O_block)\n\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T09:33:17.015296Z","iopub.execute_input":"2025-09-02T09:33:17.015570Z","iopub.status.idle":"2025-09-02T09:33:17.155650Z","shell.execute_reply.started":"2025-09-02T09:33:17.015548Z","shell.execute_reply":"2025-09-02T09:33:17.154777Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class TritonAttention(torch.autograd.Function):\n    def forward(ctx,Q,K,V,causal,softmax_scale):\n        assert Q.shape[-1]==V.shape[-1]==K.shape[-1]\n        BATCH_SIZE,SEQ_LEN,NUM_HEADS,HEAD_DIM = Q.shape\n        O = torch.empty_like(Q) #has to be same shape as Q matrix \n        stage = 3 if causal else 1\n        grid = lambda args: (\n            triton.cdiv(SEQ_LEN,args['BLOCK_SIZE_Q']), # which group of query we are working on\n            BATCH_SIZE*NUM_HEADS,  # which head in a batch are we on\n            1\n        )\n        M = torch.empty(BATCH_SIZE,NUM_HEADS,SEQ_LEN,device=Q.device,dtype=torch.float32) #this is for storing all maximums for later use in backward pass?\n\n        BLOCK_SIZE_Q = max(16,min(64, SEQ_LEN))\n        BLOCK_SIZE_KV = max(16,min(64, SEQ_LEN))\n        \n        attn_impl[grid](\n            Q=Q,\n            K=K,\n            V=V,\n            softmax_scale=softmax_scale,\n            M=M , # not mask \n            O=O,\n            stride_Q_batch = Q.stride(0),\n            stride_Q_head = Q.stride(1),\n            stride_Q_seq = Q.stride(2),\n            stride_Q_dim = Q.stride(3),\n            stride_K_batch = K.stride(0),\n            stride_K_head = K.stride(1),\n            stride_K_seq = K.stride(2),\n            stride_K_dim = K.stride(3),\n            stride_V_batch = V.stride(0),\n            stride_V_head = V.stride(1),\n            stride_V_seq = V.stride(2),\n            stride_V_dim = V.stride(3),\n            stride_O_batch = O.stride(0),\n            stride_O_head = O.stride(1),\n            stride_O_seq = O.stride(2),\n            stride_O_dim = O.stride(3),\n            BATCH_SIZE = Q.shape[0],\n            NUM_HEADS = Q.shape[1],\n            SEQ_LEN = Q.shape[2],\n            HEAD_DIM = Q.shape[3],\n            STAGE=stage,\n            BLOCK_SIZE_Q = BLOCK_SIZE_Q,\n            BLOCK_SIZE_KV = BLOCK_SIZE_KV\n        )\n        ctx.save_for_backward(Q,K,V,O,M)\n        ctx.grid = grid\n        ctx.softmax_scale = softmax_scale\n        ctx.HEAD_DIM = HEAD_DIM_K\n        \n        ctx.causal = causal\n\n        return O\n\n\n        \n        pass\n    def backward():\n        pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T09:37:41.646477Z","iopub.execute_input":"2025-08-29T09:37:41.646759Z","iopub.status.idle":"2025-08-29T09:37:41.654559Z","shell.execute_reply.started":"2025-08-29T09:37:41.646738Z","shell.execute_reply":"2025-08-29T09:37:41.653873Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def test_op(BATCH_SIZE,SEQ_LEN,NUM_HEADS,HEAD_DIM,causal,dtype=torch.float32):\n    Q = (\n        torch.empty(\n            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n        )\n        .normal_(mean=0.0, std=0.5)\n        .requires_grad_()\n    ).to(torch.float16)\n    K = (\n        torch.empty(\n            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n        )\n        .normal_(mean=0.0, std=0.5)\n        .requires_grad_()\n    ).to(torch.float16)\n    V = (\n        torch.empty(\n            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n        )\n        .normal_(mean=0.0, std=0.5)\n        .requires_grad_()\n    ).to(torch.float16)\n\n    softmax_scale = 1 / (HEAD_DIM**0.5)\n    P = torch.matmul(Q,K.transpose(2,3))* softmax_scale \n    MASK = torch.tril(torch.ones(SEQ_LEN,SEQ_LEN,device='cuda'))\n    if causal:\n        P[:,:,MASK==0] = float('-inf')\n    P = torch.softmax(P,dim=-1)\n    naive_out = torch.matmul(P,V).half()\n    triton_out  = TritonAttention.apply(Q,K,V,causal,softmax_scale).to(torch.float16)\n    assert triton.testing.assert_close(naive_out,triton_out,atol=1e-2,rtol=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T09:37:43.733369Z","iopub.execute_input":"2025-08-29T09:37:43.733669Z","iopub.status.idle":"2025-08-29T09:37:43.740278Z","shell.execute_reply.started":"2025-08-29T09:37:43.733650Z","shell.execute_reply":"2025-08-29T09:37:43.739584Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"test_op(1,32,2,32,1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}