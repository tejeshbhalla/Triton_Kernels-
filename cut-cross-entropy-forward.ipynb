{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T11:16:55.666800Z","iopub.execute_input":"2025-10-09T11:16:55.667062Z","iopub.status.idle":"2025-10-09T11:16:55.671733Z","shell.execute_reply.started":"2025-10-09T11:16:55.667045Z","shell.execute_reply":"2025-10-09T11:16:55.671148Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import triton.language as tl\nimport torch\nimport triton","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T11:16:57.588189Z","iopub.execute_input":"2025-10-09T11:16:57.588425Z","iopub.status.idle":"2025-10-09T11:16:57.592027Z","shell.execute_reply.started":"2025-10-09T11:16:57.588409Z","shell.execute_reply":"2025-10-09T11:16:57.591311Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def cross_entropy_ref(logits,targets):\n    \"\"\"\n    standard cross entropy function takes in logits of shape seq_len,vocab_size , targets of seq_len\n    \"\"\"\n    softmax_probs  = torch.nn.functional.softmax(logits,dim=-1)\n    selected_tokens_probs  = torch.gather(softmax_probs,1,targets.unsqueeze(1))\n    loss = -torch.log(selected_tokens_probs + 1e-9)  # small epsilon to avoid log(0)\n    return loss.sum()\n\n@triton.jit\ndef indexed_matmul_kernel(E_ptr,C_ptr,target_ptr,output_ptr,embed_dim,seq_len,vocab_size,stride_e_seq_len,stride_e_embed_dim,\n                         stride_c_embed_dim,stride_c_vocab_size,BLOCK_SIZE_N:tl.constexpr,BLOCK_SIZE_D:tl.constexpr):\n    pid_n = tl.program_id(0)\n    start_id  = BLOCK_SIZE_N * pid_n\n    token_range_offsets = start_id + tl.arange(0,BLOCK_SIZE_N)\n    mask_n = token_range_offsets<seq_len\n    target_indices = tl.load(target_ptr+token_range_offsets,mask=mask_n,other=0.0)\n    acc = tl.zeros((BLOCK_SIZE_N,),dtype=tl.float32)\n    for embed_id in range(tl.cdiv(embed_dim,BLOCK_SIZE_D)):\n        start_embed = embed_id * BLOCK_SIZE_D\n        embed_offsets = start_embed + tl.arange(0,BLOCK_SIZE_D)\n        embed_mask =  embed_offsets < embed_dim\n        E_ptr_block_offsets  = token_range_offsets[:,None] * stride_e_seq_len + embed_offsets[None,:]* stride_e_embed_dim\n        E_chunk = tl.load(E_ptr+E_ptr_block_offsets,mask = mask_n[:,None] & embed_mask[None,:] , other=0.0)\n\n        C_ptr_block_offsets  = embed_offsets[:,None] * stride_c_embed_dim + target_indices[None,:] * stride_c_vocab_size\n\n        C_chunk = tl.load(C_ptr+C_ptr_block_offsets, mask = embed_mask[:,None] & mask_n[None,:],other=0.0)\n\n        acc += tl.sum(E_chunk * tl.trans(C_chunk), axis=1)\n    tl.store(output_ptr+token_range_offsets,acc,mask=mask_n)\n\n        \n        \n\n\ndef cut_cross_entropy(E,C,targets):\n    \"\"\"\n    E - (seq_len,embed_dim) \n    C  - (embed_dim, vocab_size)\n    targets  - (seq_len,)\n    \n    \"\"\"\n    seq_len,embed_dim = E.shape\n    embed_dim,vocab_size = C.shape \n    target_logits = torch.empty(seq_len,device=E.device,dtype=E.dtype)\n    lse = torch.empty(seq_len,device=E.device,dtype=E.dtype)\n\n    loss = (lse - target_logits).sum()\n\n    BLOCK_SIZE_N = 128 \n    BLOCK_SIZE_D = 128\n    grid = (triton.cdiv(seq_len,BLOCK_SIZE_N),)\n    indexed_matmul_kernel[grid](\n        E,C,targets,target_logits,embed_dim,seq_len,vocab_size,E.stride(0),E.stride(1),C.stride(0),C.stride(1),BLOCK_SIZE_N=BLOCK_SIZE_N,\n        BLOCK_SIZE_D = BLOCK_SIZE_D\n    )\n\n    return loss,target_logits\n    \n\n\n\ndef test_cross_entropy_forward():\n    \"\"\"\n    Test cut cross entropy implementation \n    \n    \"\"\"\n    seq_len = 128\n    vocab_size  = 1024 \n    embed_dim = 256 \n    E  = torch.randn(seq_len,embed_dim,device='cuda',dtype=torch.float32)\n    C = torch.randn(embed_dim,vocab_size,device='cuda',dtype=torch.float32)\n    targets = torch.randint(0,vocab_size,(seq_len,),device='cuda')\n    logits  = E @ C\n    ref_loss = cross_entropy_ref(logits,targets)\n    cce_loss,target_logits  = cut_cross_entropy(E,C,targets)\n    return logits,target_logits,targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:20:13.151721Z","iopub.execute_input":"2025-10-09T09:20:13.152494Z","iopub.status.idle":"2025-10-09T09:20:13.165013Z","shell.execute_reply.started":"2025-10-09T09:20:13.152471Z","shell.execute_reply":"2025-10-09T09:20:13.164287Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"logits,logits_triton,targets = test_cross_entropy_forward()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:20:15.739008Z","iopub.execute_input":"2025-10-09T09:20:15.739591Z","iopub.status.idle":"2025-10-09T09:20:17.574625Z","shell.execute_reply.started":"2025-10-09T09:20:15.739568Z","shell.execute_reply":"2025-10-09T09:20:17.573999Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"target_logits_ref = torch.gather(logits, 1, targets.unsqueeze(1)).squeeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:20:19.719278Z","iopub.execute_input":"2025-10-09T09:20:19.719832Z","iopub.status.idle":"2025-10-09T09:20:19.725276Z","shell.execute_reply.started":"2025-10-09T09:20:19.719811Z","shell.execute_reply":"2025-10-09T09:20:19.724619Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}