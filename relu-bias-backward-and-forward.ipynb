{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T10:13:50.237566Z","iopub.execute_input":"2025-09-11T10:13:50.238337Z","iopub.status.idle":"2025-09-11T10:13:50.501530Z","shell.execute_reply.started":"2025-09-11T10:13:50.238313Z","shell.execute_reply":"2025-09-11T10:13:50.500945Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport triton\nimport triton.language as tl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T10:14:17.886230Z","iopub.execute_input":"2025-09-11T10:14:17.886948Z","iopub.status.idle":"2025-09-11T10:14:17.890718Z","shell.execute_reply.started":"2025-09-11T10:14:17.886913Z","shell.execute_reply":"2025-09-11T10:14:17.889902Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"DEVICE = torch.device('cuda')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T10:14:18.004844Z","iopub.execute_input":"2025-09-11T10:14:18.005463Z","iopub.status.idle":"2025-09-11T10:14:18.009478Z","shell.execute_reply.started":"2025-09-11T10:14:18.005431Z","shell.execute_reply":"2025-09-11T10:14:18.008837Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"@triton.jit\ndef fused_relu_kernel(x_ptr,bias_ptr,output_ptr , M:int , N :int ,BLOCK_SIZE:tl.constexpr):\n    start_id  = tl.program_id(0)\n    x_ptr += start_id* N \n    output_ptr+= start_id* N \n\n    for offset in range(0,N,BLOCK_SIZE):\n        offsets = offset  + tl.arange(0,BLOCK_SIZE)\n        mask = offsets< N \n        x_data =  tl.load(x_ptr+offsets,mask=mask,other=0.0)\n        bias_data = tl.load(bias_ptr+offsets,mask=mask,other=0.0)\n        x_data += bias_data\n        out = tl.maximum(x_data, 0.0)\n        tl.store(output_ptr+offsets,out,mask=mask)\n        \n\n\ndef triton_relu(x:torch.Tensor,bias:torch.Tensor):\n    M,N = x.shape\n    y = torch.zeros_like(x,device=x.device)\n    TOTAL_SM_MEM = 65536\n    BLOCK_SIZE = TOTAL_SM_MEM // x.element_size()\n    BLOCK_SIZE = min(256,triton.next_power_of_2(BLOCK_SIZE))\n    num_warps = 8\n    fused_relu_kernel[(M,)](\n        x,bias,y,M,N,BLOCK_SIZE=BLOCK_SIZE,num_warps=num_warps\n    )\n    return y\n\n\n@triton.jit \ndef fused_relu_backward(x_ptr, bias_ptr, grad_output_ptr, grad_x_ptr, M, N, BLOCK_SIZE: tl.constexpr):\n    start_id = tl.program_id(0)\n    x_ptr += start_id * N \n    grad_output_ptr += start_id * N \n    grad_x_ptr += start_id * N \n    \n    for offset in range(0, N, BLOCK_SIZE):\n        offsets = offset + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < N \n        x_data = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n        grad_out_data = tl.load(grad_output_ptr + offsets, mask=mask, other=0.0)\n        bias_data = tl.load(bias_ptr + offsets, mask=mask, other=0.0)\n        x_bias = x_data + bias_data\n        mask_relu = x_bias > 0 \n        grad_x_val = grad_out_data * mask_relu\n        tl.store(grad_x_ptr + offsets, grad_x_val, mask=mask)\n        \n        \n    \n    \n\n\ndef triton_relu_backward(x,bias,grad_output):\n    \"\"\"\n    shapes we get in is x (M,N)\n    bias is (N,)\n    and grad_output again is (M,N)\n    \"\"\"\n    M,N = x.shape \n    grad_x = torch.empty_like(x,device=x.device)\n    BLOCK_SIZE  = min(256,triton.next_power_of_2(N))\n    num_warps = 8 \n    fused_relu_backward[(M,)](\n        x,bias,grad_output,grad_x,M,N,BLOCK_SIZE=BLOCK_SIZE,num_warps=num_warps\n    )\n    return grad_x\n    \n    \n\n\n\nclass FusedReluFunction(torch.autograd.Function):\n    @staticmethod \n    def forward(ctx,x,bias):\n        y = triton_relu(x,bias)\n        ctx.save_for_backward(x,bias)\n        return y \n    @staticmethod\n    def backward(ctx,grad_output):\n        saved_tensors = ctx.saved_tensors\n        x,bias = saved_tensors\n        grad_x = triton_relu_backward(x,bias,grad_output)\n        grad_bias  = torch.sum(grad_x,axis=0)\n        return grad_x,grad_bias\n\ndef fused_relu(x, bias):\n    return FusedReluFunction.apply(x, bias)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T10:27:16.963895Z","iopub.execute_input":"2025-09-11T10:27:16.964158Z","iopub.status.idle":"2025-09-11T10:27:16.977151Z","shell.execute_reply.started":"2025-09-11T10:27:16.964138Z","shell.execute_reply":"2025-09-11T10:27:16.976400Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def test_relu(M,N,atol=1e-5):\n    x = torch.randn(M,N,dtype=torch.float32,device='cuda')\n    bias = torch.randn(N,dtype=torch.float32,device='cuda')\n    torch_out = torch.nn.functional.relu(x+bias)\n    tri_out = triton_relu(x,bias)\n    triton.testing.assert_close(torch_out,tri_out,atol=1e-5,rtol=1e-5)\n\ndef test_backward():\n    M,N = 4,100\n    x = torch.randn(M,N,dtype=torch.float32,device='cuda',requires_grad=True)\n    bias = torch.randn(N,dtype=torch.float32,device='cuda',requires_grad=True)\n    x_ref = x.clone().detach().requires_grad_(True)\n    bias_ref = bias.clone().detach().requires_grad_(True)\n    y_triton = fused_relu(x,bias)\n    y_torch = torch.relu(x_ref+bias_ref)\n\n    grad_output = torch.randn_like(y_triton)\n\n    y_triton.backward(grad_output)\n    y_torch.backward(grad_output)\n    \n\n    triton.testing.assert_close(x.grad,x_ref.grad,atol=1e-5,rtol=1e-5)\n    triton.testing.assert_close(bias.grad,bias_ref.grad,atol=1e-5,rtol=1e-5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T10:27:17.167681Z","iopub.execute_input":"2025-09-11T10:27:17.168398Z","iopub.status.idle":"2025-09-11T10:27:17.174471Z","shell.execute_reply.started":"2025-09-11T10:27:17.168374Z","shell.execute_reply":"2025-09-11T10:27:17.173754Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"test_backward()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T10:27:18.192713Z","iopub.execute_input":"2025-09-11T10:27:18.193015Z","iopub.status.idle":"2025-09-11T10:27:18.204352Z","shell.execute_reply.started":"2025-09-11T10:27:18.192996Z","shell.execute_reply":"2025-09-11T10:27:18.203572Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}