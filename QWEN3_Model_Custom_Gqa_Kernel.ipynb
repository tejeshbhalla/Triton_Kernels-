{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9zzdWXUizNCq"
      },
      "outputs": [],
      "source": [
        "import triton\n",
        "import torch\n",
        "import triton.language as tl\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def __attn_fwd_inner(\n",
        "    Q_data,O,L,M,K_ptr,V_ptr,k_block_ptr,v_block_ptr,q_block_idx,scale,k_stride_s,v_stride_s,BLOCK_SIZE_OQ:tl.constexpr,BLOCK_SIZE_KV:tl.constexpr,\n",
        "    DIAGONAL: tl.constexpr, qo_seq_len_offsets,kv_seq_len_offsets,kv_seq_len:tl.constexpr,head_dim:tl.constexpr,group_size:tl.constexpr\n",
        "\n",
        "):\n",
        "    if DIAGONAL:\n",
        "        lo = q_block_idx * BLOCK_SIZE_OQ\n",
        "        hi = (q_block_idx+1) * BLOCK_SIZE_OQ\n",
        "    else:\n",
        "        lo = 0\n",
        "        hi = q_block_idx * BLOCK_SIZE_OQ\n",
        "    k_block_ptr+= lo * k_stride_s\n",
        "    v_block_ptr += lo * v_stride_s\n",
        "    kv_seq_len_offsets+= lo\n",
        "\n",
        "    for start_kv in range(lo,hi,BLOCK_SIZE_KV):\n",
        "        kv_seq_len_mask = kv_seq_len_offsets < kv_seq_len\n",
        "        K_data_T = tl.load(K_ptr+k_block_ptr,mask=kv_seq_len_mask[None,:],other=0.0) #head_dim,BLOCK_SIZE_KV\n",
        "        V_data = tl.load(V_ptr+v_block_ptr,mask= kv_seq_len_mask[:,None],other=0.0).to(tl.float32) #BLOCK_SIZE_KV,head_dim\n",
        "\n",
        "        K_data_T_expanded = tl.broadcast_to(K_data_T[None, :, :], (group_size, head_dim, BLOCK_SIZE_KV))\n",
        "        V_data_expanded = tl.broadcast_to(V_data[None, :, :], (group_size, BLOCK_SIZE_KV, head_dim)).to(tl.float32)\n",
        "\n",
        "\n",
        "        S = tl.dot(Q_data,K_data_T_expanded) * scale #group_size,BLOCK_SIZE_OQ,BLOCK_SIZE_KV\n",
        "        if DIAGONAL:\n",
        "           causal_mask = qo_seq_len_offsets[:,None]>=kv_seq_len_offsets[None,:]\n",
        "           S= tl.where(causal_mask,S,-float('inf')) #group_size,BLOCK_SIZE_OQ,BLOCK_SIZE_KV #apparently causal mask auto broadcasts so no need to add a dim for group_size\n",
        "        new_max = tl.maximum(M,tl.max(S,axis=-1)) #group_size,BLOCK_SIZE_OQ\n",
        "        S = S - new_max[:,:,None]\n",
        "        P = tl.exp2(S)\n",
        "        L_new = tl.sum(P,axis=-1) #group_size,BLOCK_SIZE_OQ\n",
        "\n",
        "        correction_factor = tl.exp2(M - new_max) #group_size,BLOCK_SIZE_OQ\n",
        "        L = L * correction_factor + L_new\n",
        "        O = O * correction_factor[:,:,None]\n",
        "\n",
        "        O = tl.dot(P,V_data_expanded,acc=O) #group_size,BLOCK_SIZE_OQ,BLOCK_SIZE_KV * BLOCK_SIZE_KV,HEAD_DIM\n",
        "\n",
        "        M = new_max\n",
        "\n",
        "        k_block_ptr+= BLOCK_SIZE_KV * k_stride_s\n",
        "        v_block_ptr += BLOCK_SIZE_KV * v_stride_s\n",
        "        kv_seq_len_offsets+= BLOCK_SIZE_KV\n",
        "\n",
        "    return O,L,M\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@triton.autotune(\n",
        "    configs=[\n",
        "\n",
        "        triton.Config(\n",
        "            kwargs={\"BLOCK_SIZE_OQ\": 16, \"BLOCK_SIZE_KV\": 16},\n",
        "        ),\n",
        "    ],\n",
        "    key=[\"group_size\", \"head_dim\"],\n",
        ")\n",
        "@triton.jit\n",
        "def gqa_kernel(\n",
        "    Q_ptr, K_ptr, V_ptr, O_ptr, LSE_ptr,\n",
        "    q_stride_bs, q_stride_h, q_stride_s, q_stride_d,\n",
        "    k_stride_bs, k_stride_h, k_stride_s, k_stride_d,\n",
        "    v_stride_bs, v_stride_h, v_stride_s, v_stride_d,\n",
        "    o_stride_bs, o_stride_h, o_stride_s, o_stride_d,\n",
        "    lse_stride_bs, lse_stride_h, lse_stride_s,\n",
        "    batch_size, q_seq_len, kv_seq_len,\n",
        "    scale: tl.constexpr,\n",
        "    group_size: tl.constexpr,\n",
        "    num_heads: tl.constexpr,\n",
        "    num_kv_heads: tl.constexpr,\n",
        "    head_dim: tl.constexpr,\n",
        "    BLOCK_SIZE_OQ: tl.constexpr,\n",
        "    BLOCK_SIZE_KV: tl.constexpr,\n",
        "):\n",
        "    rln2: tl.constexpr = 1.4426950408889634\n",
        "    scale *= rln2\n",
        "    q_block_idx = tl.program_id(0)\n",
        "    bs_nkv_heads = tl.program_id(1)\n",
        "    batch_index = bs_nkv_heads // num_kv_heads\n",
        "    head_index  = bs_nkv_heads % num_kv_heads\n",
        "    Q_ptr += batch_index * q_stride_bs\n",
        "    K_ptr += batch_index * k_stride_bs + head_index * k_stride_h\n",
        "    V_ptr += batch_index * v_stride_bs + head_index * v_stride_h\n",
        "    O_ptr += batch_index * o_stride_bs\n",
        "    #move every ptr to right batch and head\n",
        "    qo_heads_offsets = group_size * head_index + tl.arange(0,group_size)\n",
        "    qo_seq_len_offsets = q_block_idx * BLOCK_SIZE_OQ + tl.arange(0,BLOCK_SIZE_OQ)\n",
        "    qo_seq_len_mask = qo_seq_len_offsets < q_seq_len #1,BLOCK_SIZE_OQ,1\n",
        "    head_dim_offsets = tl.arange(0,head_dim)\n",
        "    qo_block_ptr  = (qo_heads_offsets[:,None,None] * q_stride_h) + (qo_seq_len_offsets[None,:,None] * q_stride_s) + (head_dim_offsets[None,None,:]* q_stride_d)\n",
        "    #bs,group_size,BLOCK_SIZE_OQ,head_dim\n",
        "    q_data = tl.load(Q_ptr+qo_block_ptr,mask=qo_seq_len_mask[None,:,None],other=0.0)\n",
        "    kv_seq_len_offsets = tl.arange(0,BLOCK_SIZE_KV)\n",
        "    k_block_ptr = head_dim_offsets[:,None] * k_stride_d + kv_seq_len_offsets[None,:] * k_stride_s\n",
        "    v_block_ptr  =  kv_seq_len_offsets[:,None] * v_stride_s + head_dim_offsets[None,:] * v_stride_d\n",
        "    M = tl.full(shape=[group_size,BLOCK_SIZE_OQ],value = float(\"-inf\"),dtype=tl.float32)\n",
        "    L = tl.full(shape=[group_size,BLOCK_SIZE_OQ],value = float(0),dtype=tl.float32)\n",
        "    O = tl.zeros([group_size,BLOCK_SIZE_OQ,head_dim],dtype=tl.float32)\n",
        "\n",
        "    O,L,M = __attn_fwd_inner(\n",
        "        q_data,O,L,M,K_ptr,V_ptr,k_block_ptr,v_block_ptr,q_block_idx,scale,k_stride_s,v_stride_s,BLOCK_SIZE_OQ,BLOCK_SIZE_KV,\n",
        "        False,qo_seq_len_offsets,kv_seq_len_offsets,kv_seq_len,head_dim,group_size\n",
        "    )\n",
        "    O,L,M = __attn_fwd_inner(\n",
        "        q_data,O,L,M,K_ptr,V_ptr,k_block_ptr,v_block_ptr,q_block_idx,scale,k_stride_s,v_stride_s,BLOCK_SIZE_OQ,BLOCK_SIZE_KV,\n",
        "        True,qo_seq_len_offsets,kv_seq_len_offsets,kv_seq_len,head_dim,group_size\n",
        "    )\n",
        "\n",
        "    O = O/L[:,:,None]\n",
        "    LSE = M + tl.math.log2(L) #GROUP_SIZE,BLOCK_SIZE_OQ\n",
        "\n",
        "    LSE_offsets = batch_index*lse_stride_bs + qo_heads_offsets[:,None] * lse_stride_h + qo_seq_len_offsets\n",
        "    tl.store(LSE_ptr+LSE_offsets,LSE,mask = qo_seq_len_mask[None,:])\n",
        "\n",
        "    o_block_ptr  = (qo_heads_offsets[:,None,None] * o_stride_h) + (qo_seq_len_offsets[None,:,None] * o_stride_s) + (head_dim_offsets[None,None,:]* o_stride_d)\n",
        "    tl.store(O_ptr+o_block_ptr,O.to(tl.bfloat16),mask=qo_seq_len_mask[None,:,None])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GQA_Torch(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Custom kernel build efficient for GQA\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
        "        batch_size, num_heads, q_seq_len, head_dim = Q.shape\n",
        "        _, num_kv_heads, kv_seq_len, head_dim = K.shape\n",
        "        assert Q.shape[-1] == K.shape[-1] == V.shape[-1]\n",
        "        assert Q.shape[-1] <= 128\n",
        "        assert Q.device == K.device == V.device\n",
        "        assert K.shape == V.shape\n",
        "        assert num_heads % num_kv_heads == 0\n",
        "        O = torch.empty_like(Q)\n",
        "        LSE = torch.empty(\n",
        "            batch_size,\n",
        "            num_heads,\n",
        "            q_seq_len,\n",
        "            device=Q.device,\n",
        "            dtype=torch.bfloat16,\n",
        "        )\n",
        "        grid = lambda args: (\n",
        "            triton.cdiv(q_seq_len, args[\"BLOCK_SIZE_OQ\"]),\n",
        "            batch_size * num_kv_heads,\n",
        "        )\n",
        "        scale = 1 / math.sqrt(head_dim)\n",
        "        group_size = num_heads // num_kv_heads\n",
        "        gqa_kernel[grid](\n",
        "            Q, K, V, O, LSE,\n",
        "            Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),\n",
        "            K.stride(0), K.stride(1), K.stride(2), K.stride(3),\n",
        "            V.stride(0), V.stride(1), V.stride(2), V.stride(3),\n",
        "            O.stride(0), O.stride(1), O.stride(2), O.stride(3),\n",
        "            LSE.stride(0), LSE.stride(1), LSE.stride(2),\n",
        "            batch_size,\n",
        "            q_seq_len,\n",
        "            kv_seq_len,\n",
        "            scale,\n",
        "            group_size,\n",
        "            num_heads,\n",
        "            num_kv_heads,\n",
        "            head_dim,\n",
        "        )\n",
        "        return O\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dldO):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self,cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg['embed_dim'],cfg['hidden_dim'],dtype=cfg['dtype'],bias=False)\n",
        "        self.fc2 = nn.Linear(cfg['embed_dim'],cfg['hidden_dim'],dtype=cfg['dtype'],bias=False)\n",
        "        self.fc3 = nn.Linear(cfg['hidden_dim'],cfg['embed_dim'],dtype=cfg['dtype'],bias=False)\n",
        "    def forward(self,x):\n",
        "        x_fc1 = self.fc1(x)\n",
        "        x_fc2 = self.fc2(x)\n",
        "        x  = nn.functional.silu(x_fc1) * x_fc2\n",
        "        return self.fc3(x)\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self,emb_dim,eps=1e-6,bias=False,qwen_3_compaitable=True):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.qwen_3_compaitable = qwen_3_compaitable\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
        "\n",
        "    def forward(self,x):\n",
        "        dtype = x.dtype\n",
        "        if self.qwen_3_compaitable:\n",
        "            x = x.to(torch.float32)\n",
        "        variance = x.pow(2).mean(dim=-1,keepdim=True) #bs,seqlen,1\n",
        "        norm_x  = x * torch.rsqrt(variance+self.eps)\n",
        "        norm_x = self.scale*norm_x\n",
        "        if self.shift:\n",
        "            norm_x = norm_x + self.shift\n",
        "\n",
        "        return norm_x.to(dtype)\n",
        "\n",
        "\n",
        "def compute_rope_params(head_dim,theta_base=10000,context_length=4096,dtype=torch.float32):\n",
        "    assert head_dim%2==0 ,'embedding dim is not even'\n",
        "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype) / head_dim)) #shape headdim//2\n",
        "    positions  = torch.arange(context_length,dtype=dtype) #shape context_length\n",
        "\n",
        "    angles  = positions.unsqueeze(1) *  inv_freq.unsqueeze(0) # [[1],[2],[3],[4]] * [[1,2,3,4]] - shape - (context_length,head_dim//2)\n",
        "    angles = torch.cat((angles,angles),dim=-1) #shape - context_len,head_dim\n",
        "\n",
        "    cos = torch.cos(angles)\n",
        "    sin = torch.sin(angles)\n",
        "\n",
        "    return cos,sin\n",
        "\n",
        "def apply_rope(x,cos,sin,offset=0):\n",
        "    bs,n_heads,seq_len,head_dim = x.shape\n",
        "    assert head_dim % 2 == 0\n",
        "    x1 = x[...,:head_dim//2]\n",
        "    x2 = x[...,head_dim//2:]\n",
        "\n",
        "    cos = cos[offset:offset+seq_len,:].unsqueeze(0).unsqueeze(0)\n",
        "    sin = sin[offset:offset+seq_len,:].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    rotated = torch.cat((-x2,x1),dim=-1)\n",
        "    x_rotated = x*cos+(rotated*sin)\n",
        "\n",
        "    return x_rotated.to(dtype=x.dtype)\n",
        "\n",
        "class GQA(nn.Module):\n",
        "    def __init__(self,d_in,num_heads,num_kv_groups,head_dim=None,qk_norm=False,dtype=None):\n",
        "        super().__init__()\n",
        "        assert num_heads % num_kv_groups == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_groups = num_kv_groups\n",
        "        self.group_size = num_heads // num_kv_groups\n",
        "        if head_dim is  None:\n",
        "            assert d_in % num_heads == 0\n",
        "            head_dim = d_in // num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.d_out = num_heads * head_dim\n",
        "        self.W_query = nn.Linear(d_in,self.d_out,bias=False,dtype=dtype)\n",
        "        self.W_key = nn.Linear(d_in,num_kv_groups*head_dim,bias=False,dtype=dtype)\n",
        "        self.W_value = nn.Linear(d_in,num_kv_groups*head_dim,bias=False,dtype=dtype)\n",
        "\n",
        "        self.out_proj = nn.Linear(self.d_out,d_in,bias=False,dtype=dtype)\n",
        "\n",
        "        if qk_norm:\n",
        "            self.q_norm = RMSNorm(head_dim,eps=1e-6)\n",
        "            self.k_norm = RMSNorm(head_dim,eps=1e-6)\n",
        "        else:\n",
        "            self.q_norm = None\n",
        "            self.k_norm = None\n",
        "    def forward(self,x,mask,cos,sin,start_pos=0,cache=None):\n",
        "        batch_size,seq_len,n_dim = x.shape\n",
        "        queries  = self.W_query(x)  #-> bs,seqlen,numheads*head_dim\n",
        "        keys  = self.W_key(x)   #- > bs,seqlen,num_kv_groups*head_dim\n",
        "        values  = self.W_value(x) #-> bs,seqlen,num_kv_groups*head_dim\n",
        "\n",
        "\n",
        "        queries  = queries.view(batch_size,seq_len,self.num_heads,self.head_dim).transpose(1,2)\n",
        "        keys_new = keys.view(batch_size,seq_len,self.num_kv_groups,self.head_dim).transpose(1,2)\n",
        "        values_new = values.view(batch_size,seq_len,self.num_kv_groups,self.head_dim).transpose(1,2)\n",
        "\n",
        "        if self.q_norm:\n",
        "            queries = self.q_norm(queries)\n",
        "        if self.k_norm:\n",
        "            keys_new = self.k_norm(keys_new)\n",
        "\n",
        "        queries = apply_rope(queries,cos,sin,offset=start_pos)\n",
        "        keys_new = apply_rope(keys_new,cos,sin,offset=start_pos)\n",
        "\n",
        "        if cache is not None:\n",
        "            prev_k,prev_v = cache\n",
        "            keys = torch.cat((prev_k,keys_new),dim=2)\n",
        "            values = torch.cat((prev_v,values_new),dim=2)\n",
        "            next_cache = (keys,values)\n",
        "        else:\n",
        "            start_pos = 0\n",
        "            keys,values  = keys_new,values_new\n",
        "            next_cache = (keys,values)\n",
        "\n",
        "\n",
        "        context = GQA_Torch.apply(queries,keys,values).transpose(1,2).reshape(batch_size,seq_len,self.d_out)\n",
        "\n",
        "        return self.out_proj(context),next_cache\n",
        "\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = GQA(\n",
        "            d_in = cfg['embed_dim'],\n",
        "            num_heads = cfg['num_heads'],\n",
        "            head_dim = cfg['head_dim'],\n",
        "            num_kv_groups = cfg['num_kv_groups'],\n",
        "            qk_norm = cfg['qk_norm'],\n",
        "            dtype = cfg['dtype']\n",
        "        )\n",
        "        self.ffn = FFN(cfg)\n",
        "        self.norm1 = RMSNorm(cfg['embed_dim'],eps=1e-6)\n",
        "        self.norm2 = RMSNorm(cfg['embed_dim'],eps=1e-6)\n",
        "    def forward(self,x,mask,cos,sin,start_pos=0,cache=None):\n",
        "        skip = x\n",
        "        x = self.norm1(x)\n",
        "        x,next_cache = self.att(x,mask,cos,sin,start_pos=start_pos,cache=cache)\n",
        "        x = x+ skip\n",
        "\n",
        "        skip = x\n",
        "\n",
        "        x = self.norm2(x)\n",
        "        x = self.ffn(x)\n",
        "        x = x+skip\n",
        "\n",
        "        return x , next_cache\n",
        "\n",
        "class KVCache:\n",
        "    def __init__(self, n_layers):\n",
        "        self.cache = [None] * n_layers\n",
        "\n",
        "    def get(self, layer_idx):\n",
        "        return self.cache[layer_idx]\n",
        "\n",
        "    def update(self, layer_idx, value):\n",
        "        self.cache[layer_idx] = value\n",
        "\n",
        "    def get_all(self):\n",
        "        return self.cache\n",
        "\n",
        "    def reset(self):\n",
        "        for i in range(len(self.cache)):\n",
        "            self.cache[i] = None\n",
        "\n",
        "\n",
        "class Qwen3Model(nn.Module):\n",
        "    def __init__(self,cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['embed_dim'],dtype=cfg['dtype'])\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
        "        )\n",
        "        self.final_norm = RMSNorm(cfg['embed_dim'])\n",
        "        self.out_head = nn.Linear(cfg['embed_dim'],cfg['vocab_size'],bias=False,dtype=cfg['dtype'])\n",
        "\n",
        "\n",
        "        cos,sin = compute_rope_params(head_dim=cfg['head_dim'],theta_base=cfg['rope_base'],context_length=cfg['context_length'])\n",
        "        self.register_buffer(\"cos\", cos, persistent=False)\n",
        "        self.register_buffer(\"sin\", sin, persistent=False)\n",
        "        self.cfg = cfg\n",
        "        self.current_pos =  0\n",
        "\n",
        "    def forward(self,in_idx,cache=None):\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        x = tok_embeds\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        if cache is not None:\n",
        "            pos_start = self.current_pos\n",
        "            pos_end = seq_len + pos_start\n",
        "            self.current_pos = pos_end\n",
        "\n",
        "            mask = torch.triu(\n",
        "                torch.ones(pos_end,pos_end,device=x.device,dtype=torch.bool),diagonal=1\n",
        "            ) [pos_start:pos_end,:pos_end]   #the shape is seq_len,seq_len\n",
        "        else:\n",
        "            pos_start = 0\n",
        "            mask = torch.triu(\n",
        "                torch.ones(seq_len,seq_len,device=x.device,dtype=torch.bool),diagonal=1\n",
        "            )\n",
        "        mask = mask[None,None,:,:]\n",
        "\n",
        "        for i , block in enumerate(self.blocks):\n",
        "            blk_cache = cache.get(i) if cache else None\n",
        "            x,new_blk_cache = block(x,mask,self.cos,self.sin,start_pos=pos_start,cache=blk_cache)\n",
        "            if cache is not None:\n",
        "                cache.update(i,new_blk_cache)\n",
        "        x = self.final_norm(x)\n",
        "        logits  = self.out_head(x.to(self.cfg['dtype']))\n",
        "        return logits\n",
        "\n",
        "    def reset_kv_cache(self):\n",
        "        self.current_pos = 0\n",
        "\n",
        "\n",
        "\n",
        "QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,           # Vocabulary size\n",
        "        \"context_length\": 40_960,        # Context length that was used to train the model\n",
        "        \"embed_dim\": 1024,                 # Embedding dimension\n",
        "        \"num_heads\": 16,                   # Number of attention heads\n",
        "        \"n_layers\": 28,                  # Number of layers\n",
        "        \"hidden_dim\": 3072,              # Size of the intermediate dimension in FeedForward\n",
        "        \"head_dim\": 128,                 # Size of the heads in GQA\n",
        "        \"qk_norm\": True,                 # Whether to normalize queries and keys in GQA\n",
        "        \"num_kv_groups\": 8,                # Key-Value groups for grouped-query attention\n",
        "        \"rope_base\": 1_000_000.0,        # The base in RoPE's \"theta\"\n",
        "        \"dtype\": torch.bfloat16,         # Lower-precision dtype to reduce memory usage\n",
        "    }\n",
        "\n",
        "\n",
        "def load_weights_into_qwen(model, param_config, params):\n",
        "    def assign(left, right, tensor_name=\"unknown\"):\n",
        "        if left.shape != right.shape:\n",
        "            raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if isinstance(right, torch.Tensor):\n",
        "                left.copy_(right)\n",
        "            else:\n",
        "                left.copy_(torch.as_tensor(right, dtype=left.dtype, device=left.device))\n",
        "\n",
        "        return left\n",
        "\n",
        "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
        "\n",
        "    for l in range(param_config[\"n_layers\"]):\n",
        "        block = model.blocks[l]\n",
        "        att = block.att\n",
        "\n",
        "        # Q, K, V projections\n",
        "        att.W_query.weight = assign(\n",
        "            att.W_query.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
        "        )\n",
        "        att.W_key.weight = assign(\n",
        "            att.W_key.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
        "        )\n",
        "        att.W_value.weight = assign(\n",
        "            att.W_value.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        att.out_proj.weight = assign(\n",
        "            att.out_proj.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
        "        )\n",
        "\n",
        "        # QK norms\n",
        "        if hasattr(att, \"q_norm\") and att.q_norm is not None:\n",
        "            att.q_norm.scale = assign(\n",
        "                att.q_norm.scale,\n",
        "                params[f\"model.layers.{l}.self_attn.q_norm.weight\"],\n",
        "                f\"model.layers.{l}.self_attn.q_norm.weight\"\n",
        "            )\n",
        "        if hasattr(att, \"k_norm\") and att.k_norm is not None:\n",
        "            att.k_norm.scale = assign(\n",
        "                att.k_norm.scale,\n",
        "                params[f\"model.layers.{l}.self_attn.k_norm.weight\"],\n",
        "                f\"model.layers.{l}.self_attn.k_norm.weight\"\n",
        "            )\n",
        "\n",
        "        # Attention layernorm\n",
        "        block.norm1.scale = assign(\n",
        "            block.norm1.scale,\n",
        "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
        "            f\"model.layers.{l}.input_layernorm.weight\"\n",
        "        )\n",
        "\n",
        "        # Feedforward weights\n",
        "        block.ffn.fc1.weight = assign(\n",
        "            block.ffn.fc1.weight,\n",
        "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
        "        )\n",
        "        block.ffn.fc2.weight = assign(\n",
        "            block.ffn.fc2.weight,\n",
        "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
        "        )\n",
        "        block.ffn.fc3.weight = assign(\n",
        "            block.ffn.fc3.weight,\n",
        "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
        "        )\n",
        "        block.norm2.scale = assign(\n",
        "            block.norm2.scale,\n",
        "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
        "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
        "        )\n",
        "\n",
        "    # Final normalization and output head\n",
        "    model.final_norm.scale = assign(model.final_norm.scale, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
        "\n",
        "    if \"lm_head.weight\" in params:\n",
        "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
        "    else:\n",
        "        model.out_head.weight = model.tok_emb.weight\n",
        "        print(\"Model uses weight tying.\")\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from safetensors.torch import load_file\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "\n",
        "repo_id = f\"Qwen/Qwen3-{0.6}B\"\n",
        "local_dir = Path(repo_id).parts[-1]\n",
        "\n",
        "\n",
        "weights_file = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=\"model.safetensors\",\n",
        "        local_dir=local_dir,\n",
        "    )\n",
        "weights_dict = load_file(weights_file)\n",
        "\n",
        "\n",
        "model = Qwen3Model(QWEN3_CONFIG)\n",
        "\n",
        "\n",
        "\n",
        "load_weights_into_qwen(model, QWEN3_CONFIG, weights_dict)\n",
        "model = model.to(0)\n",
        "del weights_dict"
      ],
      "metadata": {
        "id": "b8JzJ99rzRpA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer  = AutoTokenizer.from_pretrained(repo_id)"
      ],
      "metadata": {
        "id": "qSB01HrBzbYv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "messages = [\n",
        "    {'role':\"user\",\"content\":\"Give me a short introduction to large language models with gandhiji as word used\"}\n",
        "]\n",
        "ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True)\n",
        "ids = torch.tensor(ids).unsqueeze(0)\n",
        "ids = ids.to(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "r1OSesDW1mP0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_text_from_model(\n",
        "    model,\n",
        "    token_ids,\n",
        "    max_new_tokens=512,\n",
        "    eos_token_id=None,\n",
        "    use_cache: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Streams tokens from a causal LM.\n",
        "\n",
        "    use_cache=False:\n",
        "        - No KV cache\n",
        "        - Recomputes full sequence each step (slow, but simple)\n",
        "\n",
        "    use_cache=True:\n",
        "        - Uses KV cache\n",
        "        - First pass full prompt, then token-by-token\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # ---- setup ----\n",
        "        if use_cache:\n",
        "            cache = KVCache(n_layers=len(model.blocks))\n",
        "            model.reset_kv_cache()\n",
        "        else:\n",
        "            cache = None\n",
        "\n",
        "        # ---- first forward (full prompt) ----\n",
        "        logits = model(token_ids, cache=cache)\n",
        "\n",
        "        generated = token_ids\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # pick next token\n",
        "            next_token = torch.argmax(\n",
        "                logits[:, -1, :], dim=-1, keepdim=True\n",
        "            )  # (bs, 1)\n",
        "\n",
        "            # EOS check\n",
        "            if eos_token_id is not None and torch.all(next_token == eos_token_id):\n",
        "                break\n",
        "\n",
        "            yield next_token\n",
        "\n",
        "            if use_cache:\n",
        "                # ---- cached path: feed ONLY new token ----\n",
        "                logits = model(next_token, cache=cache)\n",
        "            else:\n",
        "                # ---- no-cache path: re-feed entire sequence ----\n",
        "                generated = torch.cat([generated, next_token], dim=1)\n",
        "                logits = model(generated, cache=None)\n"
      ],
      "metadata": {
        "id": "A5ll2RcxRUJJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in stream_text_from_model(model,ids,512):\n",
        "    print(tokenizer.decode(token[0]),end='')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbe-6RIuRXRd",
        "outputId": "ae692b4d-3f3f-4a95-fc75-b9a0a172b3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, the user wants a short introduction to large language models, but they mentioned using \"Gandhiji\" as a word. First, I need to check if \"Gandhiji\" is a real person. Gandhiji is a prominent Indian philosopher and social activist, known for his ideas on non-violence and social justice. So, the user might be mixing up the name or using it incorrectly.\n",
            "\n",
            "I should clarify that \"Gandhiji\" is a real person, not a word used in the context of large language models. Maybe they meant \"Gandhi\" or \"Gandhiji\" as part of the model's name? But since the user specified \"Gandhiji,\" I need to make sure to correct that. \n",
            "\n",
            "Next, I should provide a general introduction to large language models, explaining their purpose, capabilities, and how they work. Then, mention Gandh"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UclNeRZfRnSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}